{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp\n",
      "  Downloading kfp-2.13.0.tar.gz (269 kB)\n",
      "     ---------------------------------------- 0.0/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/269.1 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 61.4/269.1 kB 149.3 kB/s eta 0:00:02\n",
      "     -------- ---------------------------- 61.4/269.1 kB 149.3 kB/s eta 0:00:02\n",
      "     -------- ---------------------------- 61.4/269.1 kB 149.3 kB/s eta 0:00:02\n",
      "     ----------- ------------------------- 81.9/269.1 kB 153.2 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 112.6/269.1 kB 187.5 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 194.6/269.1 kB 310.8 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 194.6/269.1 kB 310.8 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 194.6/269.1 kB 310.8 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 194.6/269.1 kB 310.8 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 194.6/269.1 kB 310.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 269.1/269.1 kB 318.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click<9,>=8.0.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (8.1.7)\n",
      "Collecting docstring-parser<1,>=0.7.3 (from kfp)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from kfp)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (2.38.0)\n",
      "Collecting google-cloud-storage<4,>=2.2.1 (from kfp)\n",
      "  Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting kfp-pipeline-spec==0.6.0 (from kfp)\n",
      "  Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl.metadata (293 bytes)\n",
      "Collecting kfp-server-api<2.5.0,>=2.1.0 (from kfp)\n",
      "  Downloading kfp_server_api-2.4.0.tar.gz (83 kB)\n",
      "     ---------------------------------------- 0.0/84.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.0/84.0 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<5,>=4.21.1 (from kfp)\n",
      "  Downloading protobuf-4.25.7-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<3.0.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from click<9,>=8.0.0->kfp) (0.4.6)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Collecting google-cloud-core<3.0dev,>=2.4.2 (from google-cloud-storage<4,>=2.2.1->kfp)\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage<4,>=2.2.1->kfp)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<4,>=2.2.1->kfp)\n",
      "  Downloading google_crc32c-1.7.1-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes<31,>=8.0.0->kfp)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes<31,>=8.0.0->kfp)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suvam\\anaconda3_2\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.7)\n",
      "Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl (9.1 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "   ---------------------------------------- 0.0/160.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 160.1/160.1 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
      "   ---------------------------------------- 0.0/174.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 174.9/174.9 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 14.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.4/1.7 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.7-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 13.0 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.7.1-cp312-cp312-win_amd64.whl (33 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.3/81.3 kB ? eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 294.5/294.5 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.2/50.2 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: kfp, kfp-server-api\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-2.13.0-py3-none-any.whl size=366802 sha256=0ad8b36dc4aa0beef4e845d3047ebd3f8ecda5f517f5c55e78e4a98c26fef689\n",
      "  Stored in directory: c:\\users\\suvam\\appdata\\local\\pip\\cache\\wheels\\57\\4c\\fb\\76abde234afe23a3e47811d6b7bf2fade075d84b74850e5f32\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-2.4.0-py3-none-any.whl size=116534 sha256=9bc142fe51696566bb94e2aa50a2798aa7f581ea1d99f86eb20083a9287f0b09\n",
      "  Stored in directory: c:\\users\\suvam\\appdata\\local\\pip\\cache\\wheels\\f8\\1a\\b3\\046daed09e657ff5dee9723720c924d55795a5842385de6f6e\n",
      "Successfully built kfp kfp-server-api\n",
      "Installing collected packages: protobuf, oauthlib, google-crc32c, docstring-parser, requests-oauthlib, proto-plus, kfp-server-api, kfp-pipeline-spec, googleapis-common-protos, google-resumable-media, kubernetes, google-api-core, google-cloud-core, google-cloud-storage, kfp\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed docstring-parser-0.16 google-api-core-2.24.2 google-cloud-core-2.4.3 google-cloud-storage-3.1.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 kfp-2.13.0 kfp-pipeline-spec-0.6.0 kfp-server-api-2.4.0 kubernetes-30.1.0 oauthlib-3.2.2 proto-plus-1.26.1 protobuf-4.25.7 requests-oauthlib-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics, ClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 2.13.0\n",
      "Summary: Kubeflow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: \n",
      "License: \n",
      "Location: C:\\Users\\suvam\\anaconda3_2\\Lib\\site-packages\n",
      "Requires: click, docstring-parser, google-api-core, google-auth, google-cloud-storage, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, PyYAML, requests-toolbelt, tabulate, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suvam\\anaconda3_2\\Lib\\site-packages\\kfp\\dsl\\component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component()\n",
    "def load_dataset(\n",
    "    x_train_artifact: Output[Dataset], \n",
    "    x_test_artifact: Output[Dataset],\n",
    "    y_train_artifact: Output[Dataset], \n",
    "    y_test_artifact: Output[Dataset]\n",
    "):\n",
    "    '''\n",
    "    Get dataset from Keras and load it separating input from output and train from test\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    import os\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    os.rename(\"/tmp/x_train.npy\", x_train_artifact.path)\n",
    "\n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    os.rename(\"/tmp/y_train.npy\", y_train_artifact.path)\n",
    "\n",
    "    np.save(\"/tmp/x_test.npy\", x_test)\n",
    "    os.rename(\"/tmp/x_test.npy\", x_test_artifact.path)\n",
    "\n",
    "    np.save(\"/tmp/y_test.npy\", y_test)\n",
    "    os.rename(\"/tmp/y_test.npy\", y_test_artifact.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component()\n",
    "def preprocessing(\n",
    "    metrics: Output[Metrics], \n",
    "    x_train_processed: Output[Dataset], \n",
    "    x_test_processed: Output[Dataset], \n",
    "    x_train_artifact: Input[Dataset], \n",
    "    x_test_artifact: Input[Dataset]\n",
    "):\n",
    "    '''\n",
    "    Just reshape and normalize data\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # Load data artifact store\n",
    "    x_train = np.load(x_train_artifact.path)\n",
    "    x_test = np.load(x_test_artifact.path)\n",
    "\n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API \n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1 \n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "    #Logging metrics using Kubeflow Artifacts\n",
    "    metrics.log_metric(\"Len x_train\", x_train.shape[0])\n",
    "    metrics.log_metric(\"Len x_test\", x_test.shape[0])\n",
    "\n",
    "    # save feature in artifact store \n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    os.rename(\"/tmp/x_train.npy\", x_train_processed.path)\n",
    "\n",
    "    np.save(\"/tmp/x_test.npy\", x_test)\n",
    "    os.rename(\"/tmp/x_test.npy\", x_test_processed.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\")\n",
    "def model_building(ml_model: Output[Model]):\n",
    "    '''\n",
    "    Define the model architecture\n",
    "    This way it's more simple to change the model architecture and all the steps and independent\n",
    "    '''\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    #model definition\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "    # corrected typo: MaxPoo12D ‚ûù MaxPooling2D\n",
    "    model.add(keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.save(ml_model.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['scikit-learn'])\n",
    "def model_training(\n",
    "    ml_model: Input[Model],\n",
    "    x_train_processed: Input[Dataset], x_test_processed: Input[Dataset],\n",
    "    y_train_artifact: Input[Dataset], y_test_artifact: Input[Dataset],\n",
    "    hyperparameters: dict,\n",
    "    metrics: Output[Metrics], \n",
    "    classification_metrics: Output[ClassificationMetrics], \n",
    "    model_trained: Output[Model]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with Keras API and export model metrics\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    #Load dataset\n",
    "    x_train = np.load(x_train_processed.path)\n",
    "    x_test = np.load(x_test_processed.path)\n",
    "    y_train = np.load(y_train_artifact.path)\n",
    "    y_test = np.load(y_test_artifact.path)\n",
    "\n",
    "    #load model structure\n",
    "    model = keras.models.load_model(ml_model.path)\n",
    "\n",
    "    #reading best hyperparameters from katib\n",
    "    lr = float(hyperparameters[\"lr\"])\n",
    "    no_epochs = int(hyperparameters[\"num_epochs\"])\n",
    "\n",
    "    #compile the model - we want to have a binary outcome \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    #fit the model and return the history while training \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=no_epochs,\n",
    "        batch_size=20,\n",
    "    )\n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the Loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test, y=y_test)\n",
    "\n",
    "    #build a confusion matrix\n",
    "    y_predict = model.predict(x=x_test)\n",
    "    y_predict = np.argmax(y_predict, axis=1)\n",
    "    cmatrix = confusion_matrix(y_test, y_predict).tolist()\n",
    "\n",
    "    numbers_list = [str(i) for i in range(10)]\n",
    "\n",
    "    #Log confusion matrix\n",
    "    classification_metrics.log_confusion_matrix(numbers_list, cmatrix)\n",
    "\n",
    "    #Kubeflax metrics export\n",
    "    metrics.log_metric(\"Test loss\", model_loss)\n",
    "    metrics.log_metric(\"Test accuracy\", model_accuracy)\n",
    "\n",
    "    #adding /1/ subfolder for TFServing and saving model to artifact store\n",
    "    model_trained.uri = model_trained.uri + '/1/'\n",
    "    keras.models.save_model(model, model_trained.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='mnist-classifier-dev',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def mnist_pipeline(hyperparameters: dict):\n",
    "    load_task = load_dataset()\n",
    "    preprocess_task = preprocessing(\n",
    "        x_train_artifact=load_task.outputs[\"x_train_artifact\"],\n",
    "        x_test_artifact=load_task.outputs[\"x_test_artifact\"]\n",
    "    )\n",
    "\n",
    "    model_building_task = model_building()\n",
    "    training_task = model_training(\n",
    "        ml_model=model_building_task.outputs[\"ml_model\"],\n",
    "        x_train_processed=preprocess_task.outputs[\"x_train_processed\"],\n",
    "        x_test_processed=preprocess_task.outputs[\"x_test_processed\"],\n",
    "        y_train_artifact=load_task.outputs[\"y_train_artifact\"],\n",
    "        y_test_artifact=load_task.outputs[\"y_test_artifact\"],\n",
    "        hyperparameters=hyperparameters\n",
    "    )\n",
    "\n",
    "    # TODO: serving_task = model_serving(model_trained=training_task.outputs[\"model_trained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'mnist_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM1+/3B3EWzU2gJENqMNRHA",
   "mount_file_id": "1ujeQFVEpZvH9GeLe_1uNroF4l3C2_b42",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
